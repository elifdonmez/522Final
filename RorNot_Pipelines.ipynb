{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-23 12:40:41.837214: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from typing import Dict\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import glob\n",
    "import torch\n",
    "import wget\n",
    "import zipfile\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "from mittens import GloVe as Glove\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import load_model\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "from setfit import SetFitModel, SetFitTrainer\n",
    "from datasets import load_dataset, logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import XLNetForSequenceClassification, RobertaForSequenceClassification\n",
    "from transformers import XLMRobertaForSequenceClassification, DistilBertForSequenceClassification\n",
    "from transformers import RobertaTokenizer, XLMRobertaTokenizer, DistilBertTokenizer, XLNetTokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_report(results, folds):\n",
    "    \n",
    "    \"\"\"\n",
    "    function takes the input of predicted model results on five folds and returns\n",
    "    average of weighted and macro Precision, Recall, F-1 \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    weighted_precision = []\n",
    "    weighted_recall = []\n",
    "    weighted_f1 = []\n",
    "    \n",
    "    macro_precision = []\n",
    "    macro_recall = []\n",
    "    macro_f1 = []\n",
    "    \n",
    "    for result_df in results:                        \n",
    "        res_rows = result_df.tail(3)\n",
    "\n",
    "        precision_scores =  res_rows['precision'].tolist()\n",
    "        recall_scores =  res_rows['recall'].tolist()\n",
    "        f1_scores =  res_rows['f1-score'].tolist()\n",
    "\n",
    "        precision_macro_avg =  precision_scores[1]\n",
    "        precision_weighted_avg = precision_scores[2]\n",
    "\n",
    "        recall_macro_avg =  recall_scores[1]\n",
    "        recall_weighted_avg = recall_scores[2]\n",
    "\n",
    "        fl_accuracy = f1_scores[0]\n",
    "        f1_scores_macro_avg =  f1_scores[1]\n",
    "        f1_scores_weighted_avg = f1_scores[2]\n",
    "                \n",
    "        weighted_precision.append(precision_weighted_avg)\n",
    "        weighted_recall.append(recall_weighted_avg)\n",
    "        weighted_f1.append(f1_scores_weighted_avg)\n",
    "        \n",
    "        macro_precision.append(precision_macro_avg)\n",
    "        macro_recall.append(recall_macro_avg)\n",
    "        macro_f1.append(f1_scores_macro_avg)\n",
    "                \n",
    "    weighted_average = round(sum(weighted_precision) / folds, 2), round(sum(weighted_recall) / folds, 2), round(sum(weighted_f1) / folds, 2)\n",
    "    macro_average = round(sum(macro_precision) / folds, 2), round(sum(macro_recall) / folds, 2), round(sum(macro_f1) / folds, 2)\n",
    "            \n",
    "    return weighted_average, macro_average\n",
    "\n",
    "def get_accuracy(y_actual, y_predicted):\n",
    "    \"\"\"\n",
    "    function takes the actual and predicted labels to return\n",
    "    the accuracy per fold\n",
    "    \n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for index in zip(y_actual, y_predicted):\n",
    "        \n",
    "        if index[0] == index[1]:\n",
    "                count += 1\n",
    "    topk_acc = round(count / len(y_actual), 2)\n",
    "    return topk_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML alogrithms Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ML_model_files(model_name, model_path, pca):\n",
    "    \n",
    "    \"\"\"\n",
    "    function load the ML models relevant files based \n",
    "    on the parameters given\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ML_model = pickle.load(open(model_path + '/'+ model_name + '.pickle', 'rb'))\n",
    "    if pca:\n",
    "        pca_vectorizer = pickle.load(open(model_path + 'pca_vectorizer.pickle', \"rb\"))\n",
    "    else:\n",
    "        pca_vectorizer = None\n",
    "    tfidf_vectorizer = pickle.load(open(model_path + 'tfidf_vectorizer.pickle', \"rb\"))\n",
    "    \n",
    "    return ML_model, pca_vectorizer, tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset for testing\n",
    "fold_parent = './data/dronology_five_folds/'\n",
    "\n",
    "sub_folders = []\n",
    "for folder in os.listdir(fold_parent):\n",
    "    if 'fold' in folder: \n",
    "        sub_folders.append(os.path.join(fold_parent, folder))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the value of 'model_name' with desired tradional ML model's name to get results for the model\n",
    "# to trigger more traditional ML models check the names in: model/ML_models. examples, DT, SVM, pLR etc. \n",
    "# put 'p' infront of the model name to couple our pre-processing pipeline\n",
    "model_name = 'SVM'\n",
    "PCA = True\n",
    "map_labels = {0: 'information', 1: 'requirement'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for dataset fold number : 1 on model : SVM\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.823529  1.000000  0.903226  56.000000\n",
      "requirement    1.000000  0.400000  0.571429  20.000000\n",
      "accuracy       0.842105  0.842105  0.842105   0.842105\n",
      "macro avg      0.911765  0.700000  0.737327  76.000000\n",
      "weighted avg   0.869969  0.842105  0.815911  76.000000\n",
      "--------------------------------------\n",
      "\n",
      "Results for dataset fold number : 2 on model : SVM\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.787879  0.928571  0.852459  56.000000\n",
      "requirement    0.600000  0.300000  0.400000  20.000000\n",
      "accuracy       0.763158  0.763158  0.763158   0.763158\n",
      "macro avg      0.693939  0.614286  0.626230  76.000000\n",
      "weighted avg   0.738437  0.763158  0.733391  76.000000\n",
      "--------------------------------------\n",
      "\n",
      "Results for dataset fold number : 3 on model : SVM\n",
      "\n",
      "               precision    recall  f1-score  support\n",
      "information    0.768116  0.963636  0.854839    55.00\n",
      "requirement    0.666667  0.200000  0.307692    20.00\n",
      "accuracy       0.760000  0.760000  0.760000     0.76\n",
      "macro avg      0.717391  0.581818  0.581266    75.00\n",
      "weighted avg   0.741063  0.760000  0.708933    75.00\n",
      "--------------------------------------\n",
      "\n",
      "Results for dataset fold number : 4 on model : SVM\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.805970  0.981818  0.885246  55.000000\n",
      "requirement    0.875000  0.350000  0.500000  20.000000\n",
      "accuracy       0.813333  0.813333  0.813333   0.813333\n",
      "macro avg      0.840485  0.665909  0.692623  75.000000\n",
      "weighted avg   0.824378  0.813333  0.782514  75.000000\n",
      "--------------------------------------\n",
      "\n",
      "Results for dataset fold number : 5 on model : SVM\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.776119  0.928571  0.845528  56.000000\n",
      "requirement    0.500000  0.210526  0.296296  19.000000\n",
      "accuracy       0.746667  0.746667  0.746667   0.746667\n",
      "macro avg      0.638060  0.569549  0.570912  75.000000\n",
      "weighted avg   0.706169  0.746667  0.706390  75.000000\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/base.py:348: InconsistentVersionWarning: Trying to unpickle estimator SVC from version 0.23.2 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/base.py:348: InconsistentVersionWarning: Trying to unpickle estimator PCA from version 0.23.2 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/var/folders/k1/gntrd44j2ms9z8m4589y4n440000gp/T/ipykernel_3911/192186885.py:14: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  tfidf_vectorizer = pickle.load(open(model_path + 'tfidf_vectorizer.pickle', \"rb\"))\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/base.py:348: InconsistentVersionWarning: Trying to unpickle estimator TfidfTransformer from version 0.23.2 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/base.py:348: InconsistentVersionWarning: Trying to unpickle estimator TfidfVectorizer from version 0.23.2 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/base.py:348: InconsistentVersionWarning: Trying to unpickle estimator SVC from version 0.23.2 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/base.py:348: InconsistentVersionWarning: Trying to unpickle estimator PCA from version 0.23.2 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/var/folders/k1/gntrd44j2ms9z8m4589y4n440000gp/T/ipykernel_3911/192186885.py:14: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  tfidf_vectorizer = pickle.load(open(model_path + 'tfidf_vectorizer.pickle', \"rb\"))\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/base.py:348: InconsistentVersionWarning: Trying to unpickle estimator TfidfTransformer from version 0.23.2 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/base.py:348: InconsistentVersionWarning: Trying to unpickle estimator TfidfVectorizer from version 0.23.2 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/base.py:348: InconsistentVersionWarning: Trying to unpickle estimator SVC from version 0.23.2 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/base.py:348: InconsistentVersionWarning: Trying to unpickle estimator PCA from version 0.23.2 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/var/folders/k1/gntrd44j2ms9z8m4589y4n440000gp/T/ipykernel_3911/192186885.py:14: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  tfidf_vectorizer = pickle.load(open(model_path + 'tfidf_vectorizer.pickle', \"rb\"))\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/base.py:348: InconsistentVersionWarning: Trying to unpickle estimator TfidfTransformer from version 0.23.2 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/base.py:348: InconsistentVersionWarning: Trying to unpickle estimator TfidfVectorizer from version 0.23.2 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/base.py:348: InconsistentVersionWarning: Trying to unpickle estimator SVC from version 0.23.2 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/base.py:348: InconsistentVersionWarning: Trying to unpickle estimator PCA from version 0.23.2 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/var/folders/k1/gntrd44j2ms9z8m4589y4n440000gp/T/ipykernel_3911/192186885.py:14: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  tfidf_vectorizer = pickle.load(open(model_path + 'tfidf_vectorizer.pickle', \"rb\"))\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/base.py:348: InconsistentVersionWarning: Trying to unpickle estimator TfidfTransformer from version 0.23.2 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/base.py:348: InconsistentVersionWarning: Trying to unpickle estimator TfidfVectorizer from version 0.23.2 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/base.py:348: InconsistentVersionWarning: Trying to unpickle estimator SVC from version 0.23.2 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/base.py:348: InconsistentVersionWarning: Trying to unpickle estimator PCA from version 0.23.2 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/var/folders/k1/gntrd44j2ms9z8m4589y4n440000gp/T/ipykernel_3911/192186885.py:14: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  tfidf_vectorizer = pickle.load(open(model_path + 'tfidf_vectorizer.pickle', \"rb\"))\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/base.py:348: InconsistentVersionWarning: Trying to unpickle estimator TfidfTransformer from version 0.23.2 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/base.py:348: InconsistentVersionWarning: Trying to unpickle estimator TfidfVectorizer from version 0.23.2 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load test data & make prediction\n",
    "\n",
    "ml_results = []\n",
    "avg_accuracy = []\n",
    "fold_count = 1\n",
    "\n",
    "for subs in sorted(sub_folders):\n",
    "    test_path = subs + '/test_' + 'fold_' + str(fold_count) + '.csv'\n",
    "    \n",
    "    df_test = pd.read_csv(test_path)\n",
    "    df_test['STR.REQ'] = df_test['STR.REQ'].str.lower()\n",
    "    X_test = df_test['STR.REQ']\n",
    "    y_test = df_test['class']\n",
    "    \n",
    "    model_path = './models/ML_models/' + model_name + '/fold_' + str(fold_count) + '/'\n",
    "    ML_model, pca_vectorizer, tfidf_vectorizer = load_ML_model_files(model_name, model_path, PCA)\n",
    "\n",
    "    tfidf_vecs = tfidf_vectorizer.transform(X_test)\n",
    "    normalized_tfidf = normalize(tfidf_vecs)\n",
    "\n",
    "    test_vecs = pca_vectorizer.transform(normalized_tfidf.toarray())\n",
    "    predicted_labels = ML_model.predict(test_vecs)\n",
    "    \n",
    "    evaluation_results = classification_report(y_test.tolist(), predicted_labels.tolist(), \n",
    "                                               target_names=list(map_labels.values()), \n",
    "                                               output_dict=True)\n",
    "    \n",
    "    avg_accuracy.append(get_accuracy(y_test.tolist(), predicted_labels.tolist()))\n",
    "    \n",
    "    report_df = pd.DataFrame(evaluation_results).transpose()\n",
    "    ml_results.append(report_df)\n",
    "    \n",
    "    print('\\nResults for dataset fold number :',fold_count, 'on model :', model_name)\n",
    "    print('\\n',report_df)\n",
    "    print('--------------------------------------')\n",
    "    \n",
    "    fold_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "              Precision  Recall  F1_score\n5-folds                                  \nweighted_avg       0.78    0.79      0.75\nmacro_avg          0.76    0.63      0.64\naccuracy_avg       0.78    0.78      0.78",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1_score</th>\n    </tr>\n    <tr>\n      <th>5-folds</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>weighted_avg</th>\n      <td>0.78</td>\n      <td>0.79</td>\n      <td>0.75</td>\n    </tr>\n    <tr>\n      <th>macro_avg</th>\n      <td>0.76</td>\n      <td>0.63</td>\n      <td>0.64</td>\n    </tr>\n    <tr>\n      <th>accuracy_avg</th>\n      <td>0.78</td>\n      <td>0.78</td>\n      <td>0.78</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average results of ML pipeline\n",
    "\n",
    "avg_acc_score = round(np.mean(avg_accuracy), 2)\n",
    "weighted_avg, macro_avg = get_avg_report(ml_results, folds=5)\n",
    "\n",
    "avg_scores = list([weighted_avg, macro_avg, (avg_acc_score, avg_acc_score, avg_acc_score)])\n",
    "\n",
    "final_df = pd.DataFrame([x for x in avg_scores], columns=(['Precision', 'Recall', 'F1_score']),\n",
    "                      index=['weighted_avg','macro_avg', 'accuracy_avg'])\n",
    "\n",
    "final_df.rename_axis('5-folds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Family Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer(model_name):\n",
    "    \n",
    "    \"\"\"\n",
    "    loads and returns the relevant tokenizer for passed parameter BERT model name\n",
    "    \n",
    "    \"\"\"\n",
    "    if model_name in ('BERT_base_uncased', \n",
    "                      'pBERT_base_uncased'):\n",
    "        tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\",\n",
    "                                                  do_lower_case=True)\n",
    "                \n",
    "    elif model_name in ('BERT_base_cased',\n",
    "                        'pBERT_base_cased'):\n",
    "        tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "    \n",
    "    elif model_name in ('pXLNet_base', \n",
    "                        'XLNet_base'):\n",
    "        tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
    "    \n",
    "    elif model_name in ('SciBERT_uncased', \n",
    "                        'pSciBERT_uncased'):\n",
    "        tokenizer = BertTokenizer.from_pretrained('allenai/scibert_scivocab_uncased', \n",
    "                                                  do_lower_case=True)\n",
    "    \n",
    "    elif model_name in ('pRoBERTa_base', \n",
    "                        'RoBERTa_base'):\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "    elif model_name in ('DisBERT_base_cased', \n",
    "                        'pDisBERT_base_cased'):\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
    "    \n",
    "    elif model_name in ('DisBERT_base_uncased', \n",
    "                        'pDisBERT_base_uncased'):\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "    else:\n",
    "        #'pXRBERT_base', 'XRBERT_base'\n",
    "        tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "    \n",
    "    return tokenizer\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_BERT_model(model_name, model_path):\n",
    "    \"\"\"\n",
    "    loads and returns the BERT model based on the model name and path parameters\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if model_name in ('BERT_base_uncased', 'pBERT_base_cased',\n",
    "                      'pBERT_base_uncased', 'BERT_base_cased',\n",
    "                      'SciBERT_uncased', 'pSciBERT_uncased'\n",
    "                     ):\n",
    "        model = BertForSequenceClassification.from_pretrained(model_path)                \n",
    "    elif model_name in ('pXLNet_base', \n",
    "                        'XLNet_base'\n",
    "                       ):\n",
    "        model = XLNetForSequenceClassification.from_pretrained(model_path)\n",
    "    \n",
    "    elif model_name in ('pRoBERTa_base', \n",
    "                        'RoBERTa_base'\n",
    "                       ):\n",
    "        model = RobertaForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "    elif model_name in ('DisBERT_base_cased', 'DisBERT_base_uncased',\n",
    "                        'pDisBERT_base_cased', 'pDisBERT_base_uncased'\n",
    "                       ):\n",
    "        model = DistilBertForSequenceClassification.from_pretrained(model_path)    \n",
    "    \n",
    "    else:\n",
    "        #'pXRBERT_base', 'XRBERT_base'\n",
    "        model = XLMRobertaForSequenceClassification.from_pretrained(model_path)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the value of 'model_name' with BERT model's name to get results for the model\n",
    "# to trigger more BERT models check the names in: model/BERT_family. examples, BERT_base_cased etc. \n",
    "# put 'p' infront of the model name to couple our pre-processing pipeline\n",
    "\n",
    "map_labels = {0: 'information', 1: 'requirement'}\n",
    "\n",
    "prefix = './models/DL_models/BERT_family/'\n",
    "model_name = 'DisBERT_base_uncased'\n",
    "\n",
    "fold_parent = './data/dronology_five_folds/'\n",
    "\n",
    "sub_folders = []\n",
    "for folder in os.listdir(fold_parent):\n",
    "    if 'fold' in folder: \n",
    "        sub_folders.append(os.path.join(fold_parent, folder))\n",
    "\n",
    "tokenizer = load_tokenizer(model_name)\n",
    "MAX_SEQ_LENGTH = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "DistilBertTokenizer(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 17\u001B[0m\n\u001B[1;32m     12\u001B[0m test_encodings \u001B[38;5;241m=\u001B[39m tokenizer(test_sequences, truncation\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, \n\u001B[1;32m     13\u001B[0m                            padding\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, \n\u001B[1;32m     14\u001B[0m                            max_length\u001B[38;5;241m=\u001B[39mMAX_SEQ_LENGTH, \n\u001B[1;32m     15\u001B[0m                            return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m# load model\u001B[39;00m\n\u001B[0;32m---> 17\u001B[0m model_path \u001B[38;5;241m=\u001B[39m \u001B[43mglob\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mglob\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprefix\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m/fold_\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mfold_count\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m/*\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[1;32m     18\u001B[0m bert_model \u001B[38;5;241m=\u001B[39m load_BERT_model(model_name, model_path)\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n",
      "\u001B[0;31mIndexError\u001B[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "fold_count = 1\n",
    "results = []\n",
    "avg_accuracy = []\n",
    "for subs in sorted(sub_folders):\n",
    "    test_path = subs + '/test_' + 'fold_' + str(fold_count) + '.csv'\n",
    "    \n",
    "    df_test = pd.read_csv(test_path)\n",
    "    selected_test = df_test[['STR.REQ','class']]\n",
    "\n",
    "    test_sequences = selected_test['STR.REQ'].tolist()\n",
    "\n",
    "    test_encodings = tokenizer(test_sequences, truncation=True, \n",
    "                               padding=True, \n",
    "                               max_length=MAX_SEQ_LENGTH, \n",
    "                               return_tensors=\"pt\")\n",
    "    # load model\n",
    "    model_path = glob.glob(prefix + model_name + '/fold_' + str(fold_count) + '/*')[0]\n",
    "    bert_model = load_BERT_model(model_name, model_path)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = bert_model(**test_encodings).logits\n",
    "\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    evaluation_results = classification_report(selected_test['class'].tolist(), \n",
    "                                               predictions.tolist(), \n",
    "                                               target_names=list(map_labels.values()), \n",
    "                                               output_dict=True)\n",
    "    \n",
    "    avg_accuracy.append(get_accuracy(selected_test['class'].tolist(), \n",
    "                                     predictions.tolist()))\n",
    "\n",
    "    report_df = pd.DataFrame(evaluation_results).transpose()\n",
    "    results.append(report_df)\n",
    "    \n",
    "    print('\\nResults for dataset fold number :',fold_count, 'on model :', model_name)\n",
    "    print('\\n',report_df)\n",
    "    print('--------------------------------------')\n",
    "    \n",
    "    fold_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average results of BERT model\n",
    "\n",
    "avg_acc_score = round(np.mean(avg_accuracy), 2)\n",
    "weighted_avg, macro_avg = get_avg_report(results, folds=5)\n",
    "\n",
    "avg_scores = list([weighted_avg, macro_avg, (avg_acc_score, avg_acc_score, \n",
    "                                             avg_acc_score)])\n",
    "\n",
    "final_df = pd.DataFrame([x for x in avg_scores], \n",
    "                        columns=(['Precision', 'Recall', 'F1_score']),\n",
    "                        index=['weighted_avg','macro_avg', 'accuracy_avg'])\n",
    "\n",
    "final_df.rename_axis('5-folds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fewshot Family pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(path):\n",
    "    \n",
    "    \"\"\"\n",
    "    load and return the dataset in the format fine-tuned few shot sentence-BERT \n",
    "    expects\n",
    "    \n",
    "    \"\"\"\n",
    "    dataset = load_dataset(path)\n",
    "    test_dataset = dataset['test']\n",
    "    return test_dataset\n",
    "\n",
    "def _apply_column_mapping(dataset, column_mapping: Dict[str, str]):\n",
    "    \n",
    "    \"\"\"\n",
    "    apply the column mapping required for the loaded dataset\n",
    "    \n",
    "    \"\"\"\n",
    "    dataset = dataset.rename_columns(\n",
    "        {\n",
    "            **column_mapping,\n",
    "            **{col: f\"feat_{col}\" for col in dataset.column_names if col not in column_mapping},\n",
    "        }\n",
    "    )\n",
    "    dset_format = dataset.format\n",
    "    dataset = dataset.with_format(\n",
    "        type=dset_format[\"type\"],\n",
    "        columns=dataset.column_names,\n",
    "        output_all_columns=dset_format[\"output_all_columns\"],\n",
    "        **dset_format[\"format_kwargs\"],\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "def evaluate_ST(test_data, Sent_tf_model):\n",
    "    \n",
    "    \"\"\"\n",
    "    load and evaluate the Sentence-BERT model on the given test dataset\n",
    "    \n",
    "    \"\"\"\n",
    "    eval_dataset = _apply_column_mapping(test_data, \n",
    "                                         column_mapping={\"STR.REQ\": \"text\", \"class\": \"label\"})   \n",
    "    x_test = eval_dataset[\"text\"]\n",
    "    y_test = eval_dataset[\"label\"]\n",
    "    predicted_labels = Sent_tf_model.predict(x_test)\n",
    "    \n",
    "    return predicted_labels, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_labels = {0: 'information', 1: 'requirement'}\n",
    "# replace the value of 'model_name' with desired few shot model's name to get results for the model\n",
    "# to trigger morefew shot models models check the names in: model/Fewshot_family. examples, S-BERT_10% or pMiniLM_10%. \n",
    "# put 'p' infront of the model name to couple our pre-processing pipeline\n",
    "model_name = 'pS-BERT_20%'\n",
    "\n",
    "prefix = './models/DL_models/Fewshot_family/'\n",
    "fold_parent = './data/dronology_preprocess_five_folds/'\n",
    "\n",
    "sub_folders = []\n",
    "for folder in os.listdir(fold_parent):\n",
    "    if 'fold' in folder: \n",
    "        sub_folders.append(os.path.join(fold_parent, folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_count = 1\n",
    "st_results = []\n",
    "avg_accuracy = []\n",
    "\n",
    "for subs in sorted(sub_folders):\n",
    "    test_dataset = get_dataset(subs)\n",
    "    \n",
    "    model_path = prefix + model_name + '/fold_' + str(fold_count)\n",
    "    ST_model = SetFitModel.from_pretrained(model_path)\n",
    "    \n",
    "    predicted_labels, y_test = evaluate_ST(test_dataset, ST_model)\n",
    "    \n",
    "    evaluation_results = classification_report(y_test, predicted_labels.tolist(), \n",
    "                                               target_names=list(map_labels.values()), \n",
    "                                               output_dict=True)\n",
    "    \n",
    "    avg_accuracy.append(get_accuracy(y_test, \n",
    "                                     predicted_labels.tolist()))\n",
    "\n",
    "    report_df = pd.DataFrame(evaluation_results).transpose()\n",
    "    st_results.append(report_df)\n",
    "    \n",
    "    print('\\nResults for dataset fold number :',fold_count, 'on model :', model_name)\n",
    "    print('\\n',report_df)\n",
    "    print('--------------------------------------')\n",
    "\n",
    "    fold_count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_acc_score = round(np.mean(avg_accuracy), 2)\n",
    "weighted_avg, macro_avg = get_avg_report(st_results, folds=5)\n",
    "\n",
    "avg_scores = list([weighted_avg, macro_avg, (avg_acc_score, avg_acc_score, \n",
    "                                             avg_acc_score)])\n",
    "\n",
    "final_df = pd.DataFrame([x for x in avg_scores], \n",
    "                        columns=(['Precision', 'Recall', 'F1_score']),\n",
    "                        index=['weighted_avg','macro_avg', 'accuracy_avg'])\n",
    "\n",
    "final_df.rename_axis('5-folds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_model(model_name, prefix, model_path):\n",
    "    \n",
    "    embeddings_model = None\n",
    "    if model_name in ('LSTM_FT_pre-train', 'pLSTM_FT_pre-train'):\n",
    "        \n",
    "        # Load FastText pre trained embeddings\n",
    "        if not 'wiki-news-300d-1M-subword.vec' in os.listdir(prefix):\n",
    "            \n",
    "            print('Downloading FastText pretrained model for the first time...')\n",
    "            url = 'https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M-subword.vec.zip'\n",
    "            \n",
    "            wget.download(url, out=prefix)\n",
    "            with zipfile.ZipFile(prefix + 'wiki-news-300d-1M-subword.vec.zip', 'r') as zip_ref:\n",
    "                zip_ref.extractall(prefix)\n",
    "        \n",
    "        embeddings_model = KeyedVectors.load_word2vec_format(prefix + \n",
    "                                                          'wiki-news-300d-1M-subword.vec')\n",
    "        print('\\nFastText pretrained Model loaded...')        \n",
    "\n",
    "    elif model_name in ('LSTM_GLV_pre-train', 'pLSTM_GLV_pre-train'):\n",
    "        \n",
    "        # load pre trained Glove embeddings model\n",
    "        if not 'glove.6B.100d.txt' in os.listdir(prefix):\n",
    "            url = 'https://nlp.stanford.edu/data/glove.6B.zip'\n",
    "            wget.download(url, out=prefix)\n",
    "            \n",
    "            with zipfile.ZipFile(prefix + 'glove.6B.zip', 'r') as zip_ref:\n",
    "                zip_ref.extractall(prefix)\n",
    "            \n",
    "        embeddings_model = {}\n",
    "        with open(prefix + 'glove.6B.100d.txt','r') as f:\n",
    "            for line in f:\n",
    "                \n",
    "                split_line = line.split()\n",
    "                word = split_line[0]\n",
    "                embedding = np.array(split_line[1:], dtype=np.float64)\n",
    "                embeddings_model[word] = embedding\n",
    "        \n",
    "        print('\\nPretrained Glove Model loaded...')        \n",
    "                \n",
    "    elif model_name in ('LSTM_GLV_custom', 'pLSTM_GLV_custom'):\n",
    "        \n",
    "        # load custom glove embeddings model\n",
    "        embeddings_model = Glove.load(model_path + '/glove_custom_100d.model')\n",
    "        print('\\nGlove custom pretrained Model loaded...')        \n",
    "      \n",
    "    elif model_name in ('LSTM_FT_custom', 'pLSTM_FT_custom'):\n",
    "        \n",
    "        # load custom FastText embeddings model\n",
    "        embeddings_model = KeyedVectors.load(model_path + '/fast_text.model')\n",
    "        print('\\nFastText custom Model loaded...')        \n",
    "    \n",
    "    return embeddings_model          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data_to_index(string_data, wv, model_name):\n",
    "    \n",
    "    index_data = []\n",
    "    for word in string_data:\n",
    "        if word in wv:\n",
    "            try:\n",
    "                if 'GLV_custom' in model_name:\n",
    "                    index_data.append(wv[word])\n",
    "\n",
    "                else:\n",
    "                    index_data.append(wv.vocab[word].index)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    return index_data\n",
    "\n",
    "def convert_data(train_sentences, test_sentences, modelf, model_name):\n",
    "    \n",
    "    train_data = []\n",
    "    i = 0\n",
    "    if 'GLV_custom' in model_name:\n",
    "        word_vectors = modelf.dictionary\n",
    "    else:\n",
    "        word_vectors = modelf.wv\n",
    "    \n",
    "    while i<len(train_sentences):\n",
    "        for seq in train_sentences[i]:\n",
    "            train_data.append(convert_data_to_index(train_sentences[i], word_vectors, \n",
    "                                                    model_name))\n",
    "            break\n",
    "\n",
    "        i+=1\n",
    "    \n",
    "    test_data = []\n",
    "    i = 0\n",
    "    while i<len(test_sentences):\n",
    "        for seq in test_sentences[i]:\n",
    "            test_data.append(convert_data_to_index(test_sentences[i], word_vectors, \n",
    "                                                   model_name))\n",
    "            break\n",
    "\n",
    "        i+=1\n",
    "    return train_data, test_data\n",
    "\n",
    "def pad_sequences(train_data, test_data):\n",
    "    \n",
    "    max_length_f = max([len(seq) for seq in train_data])   \n",
    "    test_padded = sequence.pad_sequences(test_data, maxlen=max_length_f, padding='pre')\n",
    "    return test_padded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset for testing\n",
    "fold_parent = './data/dronology_preprocess_five_folds/'\n",
    "#fold_parent = '../dataset/dronology_basic_data/'\n",
    "\n",
    "\n",
    "sub_folders = []\n",
    "for folder in os.listdir(fold_parent):\n",
    "    if 'fold' in folder: \n",
    "        sub_folders.append(os.path.join(fold_parent, folder))\n",
    "        \n",
    "\n",
    "map_labels = {0: 'information', 1: 'requirement'}\n",
    "prefix = './models/DL_models/LSTM_family/'\n",
    "# replace the value of 'model_name' with desired LSTM model's name to get results for the model\n",
    "# to trigger more LSTM models check the names in: model/LSTM_family. examples, LSTM_FT_custom. \n",
    "# put 'p' infront of the model name to couple our pre-processing pipeline\n",
    "model_name = 'pLSTM_FT_custom'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_count = 1\n",
    "results = []\n",
    "avg_accuracy = []\n",
    "\n",
    "for subs in sorted(sub_folders):\n",
    "    train_path = subs + '/train_' + 'fold_' + str(fold_count) + '.csv'\n",
    "    test_path = subs + '/test_' + 'fold_' + str(fold_count) + '.csv'\n",
    "    \n",
    "    test_df=pd.read_csv(test_path)\n",
    "    train_df=pd.read_csv(train_path)\n",
    "    \n",
    "    model_path = prefix + model_name + '/fold_' + str(fold_count)\n",
    "    \n",
    "    test_df['STR.REQ'] =  test_df['STR.REQ'].str.lower()\n",
    "    train_df['STR.REQ'] =  train_df['STR.REQ'].str.lower()\n",
    "    train_sentences = train_df['STR.REQ'].apply(str.split).values.tolist()\n",
    "    test_sentences = test_df['STR.REQ'].apply(str.split).values.tolist()\n",
    "\n",
    "    actual = test_df['class'].tolist()\n",
    "    \n",
    "    embeddings_model = get_embeddings_model(model_name, prefix, model_path)\n",
    "\n",
    "    if 'p' in model_name.split('_')[0]:\n",
    "        lstm_model = load_model(model_path + '/pLSTM.h5')\n",
    "    else:\n",
    "        lstm_model = load_model(model_path + '/LSTM.h5')\n",
    "    \n",
    "    train_data, test_data = convert_data(train_sentences, test_sentences, \n",
    "                                         embeddings_model, model_name)\n",
    "    test_padded = pad_sequences(train_data, test_data)\n",
    "    \n",
    "    test_padded = np.array(test_padded)              \n",
    "    \n",
    "    predictions = lstm_model.predict(test_padded)\n",
    "    sorted_predictions = (-predictions).argsort()\n",
    "    top_label_int = sorted_predictions[:, :1].flatten().tolist()\n",
    "    \n",
    "    evaluation_results = metrics.classification_report(actual, top_label_int, \n",
    "                                                       target_names=list(map_labels.values()),\n",
    "                                                       output_dict=True)\n",
    "    report_df = pd.DataFrame(evaluation_results).transpose()\n",
    "    results.append(report_df)\n",
    "    print('\\nResults for dataset fold number :',fold_count, 'on model :', model_name)\n",
    "    print('\\n',report_df)\n",
    "    print('--------------------------------------')\n",
    "    \n",
    "    avg_accuracy.append(get_accuracy(actual, top_label_int))\n",
    "    fold_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "avg_acc_score = round(np.mean(avg_accuracy), 2)\n",
    "weighted_avg, macro_avg = get_avg_report(results, folds=5)\n",
    "\n",
    "avg_scores = list([weighted_avg, macro_avg, (avg_acc_score, avg_acc_score, \n",
    "                                             avg_acc_score)])\n",
    "\n",
    "final_df = pd.DataFrame([x for x in avg_scores], \n",
    "                        columns=(['Precision', 'Recall', 'F1_score']),\n",
    "                        index=['weighted_avg','macro_avg', 'accuracy_avg'])\n",
    "\n",
    "final_df.rename_axis('5-folds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_label(ranges):\n",
    "    \"\"\"\n",
    "    returns the random label from the defined ranges of the labels\n",
    "    \"\"\"\n",
    "    temp=random.randint(1, ranges[-1][-1])\n",
    "    \n",
    "    for r in ranges:\n",
    "        if(temp>r[1] and temp<=r[-1]):\n",
    "            return r[0]\n",
    "    return None\n",
    "\n",
    "def get_ranges(df):\n",
    "    \"\"\"\n",
    "    predicts the random labels on the given test dataset\n",
    "    \n",
    "    \"\"\"\n",
    "    csum = 0\n",
    "    ranges = []\n",
    "    total_tr = len(df)\n",
    "\n",
    "    for k, v in df['class'].value_counts().to_dict().items():\n",
    "\n",
    "        csum_old = csum\n",
    "        csum += round((v/total_tr) * 100,0)\n",
    "        #print (k,\"from\", csum_old, \"to\",csum)\n",
    "        ranges.append([k, csum_old, csum])\n",
    "    \n",
    "    r_out = []\n",
    "    for row in test_df.iterrows():\n",
    "        r3labels = []\n",
    "\n",
    "        while len(r3labels)!=1:\n",
    "            rl = get_random_label(ranges)\n",
    "            if not rl in r3labels:\n",
    "                r3labels.append(rl)\n",
    "\n",
    "        r_out.append([row[1]['issueid'], row[1]['class'], r3labels])\n",
    "\n",
    "    return ranges, r_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "fold_parent = './data/dronology_five_folds/'\n",
    "\n",
    "sub_folders = []\n",
    "for folder in os.listdir(fold_parent):\n",
    "    if 'fold' in folder: \n",
    "        sub_folders.append(os.path.join(fold_parent, folder)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_count = 1\n",
    "for subs in sorted(sub_folders):\n",
    "    \n",
    "    test_path = subs + '/test_' + 'fold_' + str(fold_count) + '.csv'\n",
    "    test_df = pd.read_csv(test_path)\n",
    "    ranges, r_out = get_ranges(test_df)\n",
    "    \n",
    "    random_out = pd.DataFrame()\n",
    "    random_out['issueid'] = [i[0] for i in r_out]\n",
    "    random_out['class'] = [i[1] for i in r_out]\n",
    "    random_out['top_label'] = [i[2][0] for i in r_out]\n",
    "    evaluation_results = classification_report(random_out['class'], random_out['top_label'], \n",
    "                                               target_names=list(map_labels.values()), \n",
    "                                               output_dict=True)\n",
    "    \n",
    "    report_df = pd.DataFrame(evaluation_results).transpose()\n",
    "    print('\\nResults for fold number :',fold_count)\n",
    "    print('\\n',report_df)\n",
    "    print('--------------------------------------')\n",
    "    \n",
    "    fold_count += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
