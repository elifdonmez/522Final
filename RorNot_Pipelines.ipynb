{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from typing import Dict\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import glob\n",
    "import torch\n",
    "import wget\n",
    "import zipfile\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "from mittens import GloVe as Glove\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import load_model\n",
    "\n",
    "from setfit import SetFitModel, SetFitTrainer\n",
    "from datasets import load_dataset, logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import XLNetForSequenceClassification, RobertaForSequenceClassification\n",
    "from transformers import XLMRobertaForSequenceClassification, DistilBertForSequenceClassification\n",
    "from transformers import RobertaTokenizer, XLMRobertaTokenizer, DistilBertTokenizer, XLNetTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_report(results, folds):\n",
    "    \n",
    "    \"\"\"\n",
    "    function takes the input of predicted model results on five folds and returns\n",
    "    average of weighted and macro Precision, Recall, F-1 \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    weighted_precision = []\n",
    "    weighted_recall = []\n",
    "    weighted_f1 = []\n",
    "    \n",
    "    macro_precision = []\n",
    "    macro_recall = []\n",
    "    macro_f1 = []\n",
    "    \n",
    "    for result_df in results:                        \n",
    "        res_rows = result_df.tail(3)\n",
    "\n",
    "        precision_scores =  res_rows['precision'].tolist()\n",
    "        recall_scores =  res_rows['recall'].tolist()\n",
    "        f1_scores =  res_rows['f1-score'].tolist()\n",
    "\n",
    "        precision_macro_avg =  precision_scores[1]\n",
    "        precision_weighted_avg = precision_scores[2]\n",
    "\n",
    "        recall_macro_avg =  recall_scores[1]\n",
    "        recall_weighted_avg = recall_scores[2]\n",
    "\n",
    "        fl_accuracy = f1_scores[0]\n",
    "        f1_scores_macro_avg =  f1_scores[1]\n",
    "        f1_scores_weighted_avg = f1_scores[2]\n",
    "                \n",
    "        weighted_precision.append(precision_weighted_avg)\n",
    "        weighted_recall.append(recall_weighted_avg)\n",
    "        weighted_f1.append(f1_scores_weighted_avg)\n",
    "        \n",
    "        macro_precision.append(precision_macro_avg)\n",
    "        macro_recall.append(recall_macro_avg)\n",
    "        macro_f1.append(f1_scores_macro_avg)\n",
    "                \n",
    "    weighted_average = round(sum(weighted_precision) / folds, 2), round(sum(weighted_recall) / folds, 2), round(sum(weighted_f1) / folds, 2)\n",
    "    macro_average = round(sum(macro_precision) / folds, 2), round(sum(macro_recall) / folds, 2), round(sum(macro_f1) / folds, 2)\n",
    "            \n",
    "    return weighted_average, macro_average\n",
    "\n",
    "def get_accuracy(y_actual, y_predicted):\n",
    "    \"\"\"\n",
    "    function takes the actual and predicted labels to return\n",
    "    the accuracy per fold\n",
    "    \n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for index in zip(y_actual, y_predicted):\n",
    "        \n",
    "        if index[0] == index[1]:\n",
    "                count += 1\n",
    "    topk_acc = round(count / len(y_actual), 2)\n",
    "    return topk_acc\n",
    "\n",
    "\n",
    "def get_f1_score(y_actual, y_predicted):\n",
    "    \"\"\"\n",
    "    function takes the actual and predicted labels to return\n",
    "    the accuracy per fold\n",
    "\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for index in zip(y_actual, y_predicted):\n",
    "\n",
    "        if index[0] == index[1]:\n",
    "                count += 1\n",
    "    topk_acc = round(count / len(y_actual), 2)\n",
    "    return topk_acc\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML alogrithms Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ML_model_files(model_name, model_path, pca=None):\n",
    "\n",
    "    \"\"\"\n",
    "    function load the ML models relevant files based\n",
    "    on the parameters given\n",
    "\n",
    "    \"\"\"\n",
    "    ML_model = pickle.load(open(model_path + '/'+ model_name + '.pickle', 'rb'))\n",
    "    if pca:\n",
    "        pca_vectorizer = pickle.load(open(model_path + 'pca_vectorizer.pickle', 'rb'))\n",
    "    else:\n",
    "        pca_vectorizer = None\n",
    "    tfidf_vectorizer = pickle.load(open(model_path + 'tfidf_vectorizer.pickle', 'rb'))\n",
    "\n",
    "    return ML_model, pca_vectorizer, tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset for testing\n",
    "fold_parent = './data/dronology_five_folds/'\n",
    "\n",
    "sub_folders = []\n",
    "for folder in os.listdir(fold_parent):\n",
    "    if 'fold' in folder: \n",
    "        sub_folders.append(os.path.join(fold_parent, folder))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the value of 'model_name' with desired tradional ML model's name to get results for the model\n",
    "# to trigger more traditional ML models check the names in: model/ML_models. examples, DT, SVM, pLR etc. \n",
    "# put 'p' infront of the model name to couple our pre-processing pipeline\n",
    "model_name = 'LR'\n",
    "PCA = True\n",
    "map_labels = {0: 'information', 1: 'requirement'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for dataset fold number : 2 on model : LR\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.756757  1.000000  0.861538  56.000000\n",
      "requirement    1.000000  0.100000  0.181818  20.000000\n",
      "accuracy       0.763158  0.763158  0.763158   0.763158\n",
      "macro avg      0.878378  0.550000  0.521678  76.000000\n",
      "weighted avg   0.820768  0.763158  0.682665  76.000000\n",
      "--------------------------------------\n",
      "\n",
      "Results for dataset fold number : 3 on model : LR\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.767123  1.000000  0.868217  56.000000\n",
      "requirement    1.000000  0.150000  0.260870  20.000000\n",
      "accuracy       0.776316  0.776316  0.776316   0.776316\n",
      "macro avg      0.883562  0.575000  0.564543  76.000000\n",
      "weighted avg   0.828407  0.776316  0.708389  76.000000\n",
      "--------------------------------------\n",
      "\n",
      "Results for dataset fold number : 4 on model : LR\n",
      "\n",
      "               precision    recall  f1-score  support\n",
      "information    0.729730  0.981818  0.837209    55.00\n",
      "requirement    0.000000  0.000000  0.000000    20.00\n",
      "accuracy       0.720000  0.720000  0.720000     0.72\n",
      "macro avg      0.364865  0.490909  0.418605    75.00\n",
      "weighted avg   0.535135  0.720000  0.613953    75.00\n",
      "--------------------------------------\n",
      "\n",
      "Results for dataset fold number : 5 on model : LR\n",
      "\n",
      "               precision  recall  f1-score  support\n",
      "information    0.753425    1.00  0.859375    55.00\n",
      "requirement    1.000000    0.10  0.181818    20.00\n",
      "accuracy       0.760000    0.76  0.760000     0.76\n",
      "macro avg      0.876712    0.55  0.520597    75.00\n",
      "weighted avg   0.819178    0.76  0.678693    75.00\n",
      "--------------------------------------\n",
      "\n",
      "Results for dataset fold number : 6 on model : LR\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.760563  0.964286  0.850394  56.000000\n",
      "requirement    0.500000  0.105263  0.173913  19.000000\n",
      "accuracy       0.746667  0.746667  0.746667   0.746667\n",
      "macro avg      0.630282  0.534774  0.512153  75.000000\n",
      "weighted avg   0.694554  0.746667  0.679019  75.000000\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# load test data & make prediction\n",
    "\n",
    "ml_results = []\n",
    "avg_accuracy = []\n",
    "fold_count = 1\n",
    "lr_overall_predicted_labels = []\n",
    "\n",
    "for subs in sorted(sub_folders):\n",
    "    test_path = subs + '/test_' + 'fold_' + str(fold_count) + '.csv'\n",
    "\n",
    "    df_test = pd.read_csv(test_path)\n",
    "    df_test['STR.REQ'] = df_test['STR.REQ'].str.lower()\n",
    "    X_test = df_test['STR.REQ']\n",
    "    y_test = df_test['class']\n",
    "\n",
    "    model_path = './models/ML_models/' + model_name + '/fold_' + str(fold_count) + '/'\n",
    "    ML_model, pca_vectorizer, tfidf_vectorizer = load_ML_model_files(model_name, model_path, PCA)\n",
    "\n",
    "    tfidf_vecs = tfidf_vectorizer.transform(X_test)\n",
    "    normalized_tfidf = normalize(tfidf_vecs)\n",
    "\n",
    "    test_vecs = pca_vectorizer.transform(normalized_tfidf.toarray())\n",
    "    predicted_labels = ML_model.predict(test_vecs)\n",
    "\n",
    "    evaluation_results = classification_report(y_test.tolist(), predicted_labels.tolist(),\n",
    "                                               target_names=list(map_labels.values()),\n",
    "                                               output_dict=True)\n",
    "\n",
    "    avg_accuracy.append(get_accuracy(y_test.tolist(), predicted_labels.tolist()))\n",
    "\n",
    "    report_df = pd.DataFrame(evaluation_results).transpose()\n",
    "    ml_results.append(report_df)\n",
    "\n",
    "    lr_overall_predicted_labels.append(predicted_labels)\n",
    "    fold_count += 1\n",
    "\n",
    "    print('\\nResults for dataset fold number :',fold_count, 'on model :', model_name)\n",
    "    print('\\n',report_df)\n",
    "    print('--------------------------------------')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "outputs": [],
   "source": [
    "# replace the value of 'model_name' with desired tradional ML model's name to get results for the model\n",
    "# to trigger more traditional ML models check the names in: model/ML_models. examples, DT, SVM, pLR etc.\n",
    "# put 'p' infront of the model name to couple our pre-processing pipeline\n",
    "model_name = 'pLR'\n",
    "PCA = True\n",
    "map_labels = {0: 'information', 1: 'requirement'}\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      " [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# load test data & make prediction\n",
    "\n",
    "ml_results = []\n",
    "avg_accuracy = []\n",
    "fold_count = 1\n",
    "plr_overall_predicted_labels = []\n",
    "\n",
    "for subs in sorted(sub_folders):\n",
    "    test_path = subs + '/test_' + 'fold_' + str(fold_count) + '.csv'\n",
    "\n",
    "    df_test = pd.read_csv(test_path)\n",
    "    df_test['STR.REQ'] = df_test['STR.REQ'].str.lower()\n",
    "    X_test = df_test['STR.REQ']\n",
    "    y_test = df_test['class']\n",
    "\n",
    "    model_path = './models/ML_models/' + model_name + '/fold_' + str(fold_count) + '/'\n",
    "    ML_model, pca_vectorizer, tfidf_vectorizer = load_ML_model_files(model_name, model_path, PCA)\n",
    "\n",
    "    tfidf_vecs = tfidf_vectorizer.transform(X_test)\n",
    "    normalized_tfidf = normalize(tfidf_vecs)\n",
    "\n",
    "    test_vecs = pca_vectorizer.transform(normalized_tfidf.toarray())\n",
    "    predicted_labels = ML_model.predict(test_vecs)\n",
    "\n",
    "    evaluation_results = classification_report(y_test.tolist(), predicted_labels.tolist(),\n",
    "                                               target_names=list(map_labels.values()),\n",
    "                                               output_dict=True)\n",
    "\n",
    "    avg_accuracy.append(get_accuracy(y_test.tolist(), predicted_labels.tolist()))\n",
    "\n",
    "    report_df = pd.DataFrame(evaluation_results).transpose()\n",
    "    ml_results.append(report_df)\n",
    "\n",
    "    plr_overall_predicted_labels.append(predicted_labels)\n",
    "    print('\\n',predicted_labels.tolist())\n",
    "\n",
    "    fold_count += 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "outputs": [],
   "source": [
    "# replace the value of 'model_name' with desired tradional ML model's name to get results for the model\n",
    "# to trigger more traditional ML models check the names in: model/ML_models. examples, DT, SVM, pLR etc.\n",
    "# put 'p' infront of the model name to couple our pre-processing pipeline\n",
    "model_name = 'SVM'\n",
    "PCA = True\n",
    "map_labels = {0: 'information', 1: 'requirement'}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for dataset fold number : 1 on model : SVM\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.823529  1.000000  0.903226  56.000000\n",
      "requirement    1.000000  0.400000  0.571429  20.000000\n",
      "accuracy       0.842105  0.842105  0.842105   0.842105\n",
      "macro avg      0.911765  0.700000  0.737327  76.000000\n",
      "weighted avg   0.869969  0.842105  0.815911  76.000000\n",
      "--------------------------------------\n",
      "\n",
      "Results for dataset fold number : 2 on model : SVM\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.787879  0.928571  0.852459  56.000000\n",
      "requirement    0.600000  0.300000  0.400000  20.000000\n",
      "accuracy       0.763158  0.763158  0.763158   0.763158\n",
      "macro avg      0.693939  0.614286  0.626230  76.000000\n",
      "weighted avg   0.738437  0.763158  0.733391  76.000000\n",
      "--------------------------------------\n",
      "\n",
      "Results for dataset fold number : 3 on model : SVM\n",
      "\n",
      "               precision    recall  f1-score  support\n",
      "information    0.768116  0.963636  0.854839    55.00\n",
      "requirement    0.666667  0.200000  0.307692    20.00\n",
      "accuracy       0.760000  0.760000  0.760000     0.76\n",
      "macro avg      0.717391  0.581818  0.581266    75.00\n",
      "weighted avg   0.741063  0.760000  0.708933    75.00\n",
      "--------------------------------------\n",
      "\n",
      "Results for dataset fold number : 4 on model : SVM\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.805970  0.981818  0.885246  55.000000\n",
      "requirement    0.875000  0.350000  0.500000  20.000000\n",
      "accuracy       0.813333  0.813333  0.813333   0.813333\n",
      "macro avg      0.840485  0.665909  0.692623  75.000000\n",
      "weighted avg   0.824378  0.813333  0.782514  75.000000\n",
      "--------------------------------------\n",
      "\n",
      "Results for dataset fold number : 5 on model : SVM\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.776119  0.928571  0.845528  56.000000\n",
      "requirement    0.500000  0.210526  0.296296  19.000000\n",
      "accuracy       0.746667  0.746667  0.746667   0.746667\n",
      "macro avg      0.638060  0.569549  0.570912  75.000000\n",
      "weighted avg   0.706169  0.746667  0.706390  75.000000\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# load test data & make prediction\n",
    "\n",
    "ml_results = []\n",
    "avg_accuracy = []\n",
    "fold_count = 1\n",
    "svm_overall_predicted_labels = []\n",
    "\n",
    "\n",
    "for subs in sorted(sub_folders):\n",
    "    test_path = subs + '/test_' + 'fold_' + str(fold_count) + '.csv'\n",
    "\n",
    "    df_test = pd.read_csv(test_path)\n",
    "    df_test['STR.REQ'] = df_test['STR.REQ'].str.lower()\n",
    "    X_test = df_test['STR.REQ']\n",
    "    y_test = df_test['class']\n",
    "\n",
    "    model_path = './models/ML_models/' + model_name + '/fold_' + str(fold_count) + '/'\n",
    "    ML_model, pca_vectorizer, tfidf_vectorizer = load_ML_model_files(model_name, model_path, PCA)\n",
    "\n",
    "    tfidf_vecs = tfidf_vectorizer.transform(X_test)\n",
    "    normalized_tfidf = normalize(tfidf_vecs)\n",
    "\n",
    "    test_vecs = pca_vectorizer.transform(normalized_tfidf.toarray())\n",
    "    predicted_labels = ML_model.predict(test_vecs)\n",
    "\n",
    "    evaluation_results = classification_report(y_test.tolist(), predicted_labels.tolist(),\n",
    "                                               target_names=list(map_labels.values()),\n",
    "                                               output_dict=True)\n",
    "\n",
    "    avg_accuracy.append(get_accuracy(y_test.tolist(), predicted_labels.tolist()))\n",
    "\n",
    "    report_df = pd.DataFrame(evaluation_results).transpose()\n",
    "    ml_results.append(report_df)\n",
    "\n",
    "    svm_overall_predicted_labels.append(predicted_labels)\n",
    "\n",
    "    print('\\nResults for dataset fold number :',fold_count, 'on model :', model_name)\n",
    "    print('\\n',report_df)\n",
    "    print('--------------------------------------')\n",
    "\n",
    "    fold_count += 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "outputs": [],
   "source": [
    "# replace the value of 'model_name' with desired tradional ML model's name to get results for the model\n",
    "# to trigger more traditional ML models check the names in: model/ML_models. examples, DT, SVM, pLR etc.\n",
    "# put 'p' infront of the model name to couple our pre-processing pipeline\n",
    "model_name = 'pSVM'\n",
    "PCA = True\n",
    "map_labels = {0: 'information', 1: 'requirement'}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for dataset fold number : 1 on model : pSVM\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.794118  0.964286  0.870968  56.000000\n",
      "requirement    0.750000  0.300000  0.428571  20.000000\n",
      "accuracy       0.789474  0.789474  0.789474   0.789474\n",
      "macro avg      0.772059  0.632143  0.649770  76.000000\n",
      "weighted avg   0.782508  0.789474  0.754548  76.000000\n",
      "--------------------------------------\n",
      "\n",
      "Results for dataset fold number : 2 on model : pSVM\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.782609  0.964286  0.864000  56.000000\n",
      "requirement    0.714286  0.250000  0.370370  20.000000\n",
      "accuracy       0.776316  0.776316  0.776316   0.776316\n",
      "macro avg      0.748447  0.607143  0.617185  76.000000\n",
      "weighted avg   0.764629  0.776316  0.734097  76.000000\n",
      "--------------------------------------\n",
      "\n",
      "Results for dataset fold number : 3 on model : pSVM\n",
      "\n",
      "               precision    recall  f1-score  support\n",
      "information    0.760563  0.981818  0.857143    55.00\n",
      "requirement    0.750000  0.150000  0.250000    20.00\n",
      "accuracy       0.760000  0.760000  0.760000     0.76\n",
      "macro avg      0.755282  0.565909  0.553571    75.00\n",
      "weighted avg   0.757746  0.760000  0.695238    75.00\n",
      "--------------------------------------\n",
      "\n",
      "Results for dataset fold number : 4 on model : pSVM\n",
      "\n",
      "               precision    recall  f1-score  support\n",
      "information    0.760563  0.981818  0.857143    55.00\n",
      "requirement    0.750000  0.150000  0.250000    20.00\n",
      "accuracy       0.760000  0.760000  0.760000     0.76\n",
      "macro avg      0.755282  0.565909  0.553571    75.00\n",
      "weighted avg   0.757746  0.760000  0.695238    75.00\n",
      "--------------------------------------\n",
      "\n",
      "Results for dataset fold number : 5 on model : pSVM\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.760563  0.964286  0.850394  56.000000\n",
      "requirement    0.500000  0.105263  0.173913  19.000000\n",
      "accuracy       0.746667  0.746667  0.746667   0.746667\n",
      "macro avg      0.630282  0.534774  0.512153  75.000000\n",
      "weighted avg   0.694554  0.746667  0.679019  75.000000\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# load test data & make prediction\n",
    "\n",
    "ml_results = []\n",
    "avg_accuracy = []\n",
    "fold_count = 1\n",
    "psvm_overall_predicted_labels = []\n",
    "\n",
    "for subs in sorted(sub_folders):\n",
    "    test_path = subs + '/test_' + 'fold_' + str(fold_count) + '.csv'\n",
    "\n",
    "    df_test = pd.read_csv(test_path)\n",
    "    df_test['STR.REQ'] = df_test['STR.REQ'].str.lower()\n",
    "    X_test = df_test['STR.REQ']\n",
    "    y_test = df_test['class']\n",
    "\n",
    "    model_path = './models/ML_models/' + model_name + '/fold_' + str(fold_count) + '/'\n",
    "    ML_model, pca_vectorizer, tfidf_vectorizer = load_ML_model_files(model_name, model_path, PCA)\n",
    "\n",
    "    tfidf_vecs = tfidf_vectorizer.transform(X_test)\n",
    "    normalized_tfidf = normalize(tfidf_vecs)\n",
    "\n",
    "    test_vecs = pca_vectorizer.transform(normalized_tfidf.toarray())\n",
    "    predicted_labels = ML_model.predict(test_vecs)\n",
    "\n",
    "    evaluation_results = classification_report(y_test.tolist(), predicted_labels.tolist(),\n",
    "                                               target_names=list(map_labels.values()),\n",
    "                                               output_dict=True)\n",
    "\n",
    "    avg_accuracy.append(get_accuracy(y_test.tolist(), predicted_labels.tolist()))\n",
    "\n",
    "    report_df = pd.DataFrame(evaluation_results).transpose()\n",
    "    ml_results.append(report_df)\n",
    "\n",
    "    psvm_overall_predicted_labels.append(predicted_labels)\n",
    "\n",
    "    print('\\nResults for dataset fold number :',fold_count, 'on model :', model_name)\n",
    "    print('\\n',report_df)\n",
    "    print('--------------------------------------')\n",
    "\n",
    "\n",
    "    fold_count += 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Family Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer(model_name):\n",
    "    \n",
    "    \"\"\"\n",
    "    loads and returns the relevant tokenizer for passed parameter BERT model name\n",
    "    \n",
    "    \"\"\"\n",
    "    if model_name in ('BERT_base_uncased', \n",
    "                      'pBERT_base_uncased'):\n",
    "        tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\",\n",
    "                                                  do_lower_case=True)\n",
    "                \n",
    "    elif model_name in ('BERT_base_cased',\n",
    "                        'pBERT_base_cased'):\n",
    "        tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "    \n",
    "    elif model_name in ('pXLNet_base', \n",
    "                        'XLNet_base'):\n",
    "        tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
    "    \n",
    "    elif model_name in ('SciBERT_uncased', \n",
    "                        'pSciBERT_uncased'):\n",
    "        tokenizer = BertTokenizer.from_pretrained('allenai/scibert_scivocab_uncased', \n",
    "                                                  do_lower_case=True)\n",
    "    \n",
    "    elif model_name in ('pRoBERTa_base', \n",
    "                        'RoBERTa_base'):\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "    elif model_name in ('DisBERT_base_cased', \n",
    "                        'pDisBERT_base_cased'):\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
    "    \n",
    "    elif model_name in ('DisBERT_base_uncased', \n",
    "                        'pDisBERT_base_uncased'):\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "    else:\n",
    "        #'pXRBERT_base', 'XRBERT_base'\n",
    "        tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "    \n",
    "    return tokenizer\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_BERT_model(model_name, model_path):\n",
    "    \"\"\"\n",
    "    loads and returns the BERT model based on the model name and path parameters\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if model_name in ('BERT_base_uncased', 'pBERT_base_cased',\n",
    "                      'pBERT_base_uncased', 'BERT_base_cased',\n",
    "                      'SciBERT_uncased', 'pSciBERT_uncased'\n",
    "                     ):\n",
    "        model = BertForSequenceClassification.from_pretrained(model_path)                \n",
    "    elif model_name in ('pXLNet_base', \n",
    "                        'XLNet_base'\n",
    "                       ):\n",
    "        model = XLNetForSequenceClassification.from_pretrained(model_path)\n",
    "    \n",
    "    elif model_name in ('pRoBERTa_base', \n",
    "                        'RoBERTa_base'\n",
    "                       ):\n",
    "        model = RobertaForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "    elif model_name in ('DisBERT_base_cased', 'DisBERT_base_uncased',\n",
    "                        'pDisBERT_base_cased', 'pDisBERT_base_uncased'\n",
    "                       ):\n",
    "        model = DistilBertForSequenceClassification.from_pretrained(model_path)    \n",
    "    \n",
    "    else:\n",
    "        #'pXRBERT_base', 'XRBERT_base'\n",
    "        model = XLMRobertaForSequenceClassification.from_pretrained(model_path)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the value of 'model_name' with BERT model's name to get results for the model\n",
    "# to trigger more BERT models check the names in: model/BERT_family. examples, BERT_base_cased etc. \n",
    "# put 'p' infront of the model name to couple our pre-processing pipeline\n",
    "\n",
    "map_labels = {0: 'information', 1: 'requirement'}\n",
    "\n",
    "prefix = './models/DL_models/BERT_family/'\n",
    "model_name = 'BERT_base_uncased'\n",
    "\n",
    "fold_parent = './data/dronology_five_folds/'\n",
    "\n",
    "sub_folders = []\n",
    "for folder in os.listdir(fold_parent):\n",
    "    if 'fold' in folder: \n",
    "        sub_folders.append(os.path.join(fold_parent, folder))\n",
    "\n",
    "tokenizer = load_tokenizer(model_name)\n",
    "MAX_SEQ_LENGTH = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "BertTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}"
     },
     "execution_count": 593,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for dataset fold number : 1 on model : BERT_base_uncased\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.760563  0.964286  0.850394  56.000000\n",
      "requirement    0.500000  0.105263  0.173913  19.000000\n",
      "accuracy       0.746667  0.746667  0.746667   0.746667\n",
      "macro avg      0.630282  0.534774  0.512153  75.000000\n",
      "weighted avg   0.694554  0.746667  0.679019  75.000000\n",
      "--------------------------------------\n",
      "\n",
      "Results for dataset fold number : 2 on model : BERT_base_uncased\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.760563  0.964286  0.850394  56.000000\n",
      "requirement    0.500000  0.105263  0.173913  19.000000\n",
      "accuracy       0.746667  0.746667  0.746667   0.746667\n",
      "macro avg      0.630282  0.534774  0.512153  75.000000\n",
      "weighted avg   0.694554  0.746667  0.679019  75.000000\n",
      "--------------------------------------\n",
      "\n",
      "Results for dataset fold number : 3 on model : BERT_base_uncased\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.760563  0.964286  0.850394  56.000000\n",
      "requirement    0.500000  0.105263  0.173913  19.000000\n",
      "accuracy       0.746667  0.746667  0.746667   0.746667\n",
      "macro avg      0.630282  0.534774  0.512153  75.000000\n",
      "weighted avg   0.694554  0.746667  0.679019  75.000000\n",
      "--------------------------------------\n",
      "\n",
      "Results for dataset fold number : 4 on model : BERT_base_uncased\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.760563  0.964286  0.850394  56.000000\n",
      "requirement    0.500000  0.105263  0.173913  19.000000\n",
      "accuracy       0.746667  0.746667  0.746667   0.746667\n",
      "macro avg      0.630282  0.534774  0.512153  75.000000\n",
      "weighted avg   0.694554  0.746667  0.679019  75.000000\n",
      "--------------------------------------\n",
      "\n",
      "Results for dataset fold number : 5 on model : BERT_base_uncased\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.760563  0.964286  0.850394  56.000000\n",
      "requirement    0.500000  0.105263  0.173913  19.000000\n",
      "accuracy       0.746667  0.746667  0.746667   0.746667\n",
      "macro avg      0.630282  0.534774  0.512153  75.000000\n",
      "weighted avg   0.694554  0.746667  0.679019  75.000000\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "fold_count = 1\n",
    "results = []\n",
    "avg_accuracy = []\n",
    "bert_overall_predicted_labels = []\n",
    "\n",
    "for subs in sorted(sub_folders):\n",
    "    test_path = subs + '/test_' + 'fold_' + str(fold_count) + '.csv'\n",
    "\n",
    "    df_test = pd.read_csv(test_path)\n",
    "    selected_test = df_test[['STR.REQ','class']]\n",
    "\n",
    "    test_sequences = selected_test['STR.REQ'].tolist()\n",
    "\n",
    "    test_encodings = tokenizer(test_sequences, truncation=True,\n",
    "                               padding=True,\n",
    "                               max_length=MAX_SEQ_LENGTH,\n",
    "                               return_tensors=\"pt\")\n",
    "\n",
    "    # load model\n",
    "    model_path = glob.glob(prefix + model_name + '/fold_' + str(fold_count) + '/*')[0]\n",
    "    bert_model = load_BERT_model(model_name, model_path)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = bert_model(**test_encodings).logits\n",
    "\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    evaluation_results = classification_report(selected_test['class'].tolist(),\n",
    "                                               predictions.tolist(),\n",
    "                                               target_names=list(map_labels.values()),\n",
    "                                               output_dict=True)\n",
    "\n",
    "    avg_accuracy.append(get_accuracy(selected_test['class'].tolist(),\n",
    "                                     predictions.tolist()))\n",
    "\n",
    "    report_df = pd.DataFrame(evaluation_results).transpose()\n",
    "    results.append(report_df)\n",
    "\n",
    "    bert_overall_predicted_labels.append(predicted_labels)\n",
    "\n",
    "    evaluation_results = classification_report(y_test.tolist(), predicted_labels.tolist(),\n",
    "                                               target_names=list(map_labels.values()),\n",
    "                                               output_dict=True)\n",
    "\n",
    "    avg_accuracy.append(get_accuracy(y_test.tolist(), predicted_labels.tolist()))\n",
    "\n",
    "    report_df = pd.DataFrame(evaluation_results).transpose()\n",
    "    ml_results.append(report_df)\n",
    "\n",
    "    print('\\nResults for dataset fold number :',fold_count, 'on model :', model_name)\n",
    "    print('\\n',report_df)\n",
    "    print('--------------------------------------')\n",
    "\n",
    "\n",
    "    fold_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for dataset fold number : 1 on model : BERT_base_uncased\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    1.000000  0.928571  0.962963  56.000000\n",
      "requirement    0.833333  1.000000  0.909091  20.000000\n",
      "accuracy       0.947368  0.947368  0.947368   0.947368\n",
      "macro avg      0.916667  0.964286  0.936027  76.000000\n",
      "weighted avg   0.956140  0.947368  0.948786  76.000000\n",
      "--------------------------------------\n",
      "\n",
      "Results for dataset fold number : 2 on model : BERT_base_uncased\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.828125  0.946429  0.883333  56.000000\n",
      "requirement    0.750000  0.450000  0.562500  20.000000\n",
      "accuracy       0.815789  0.815789  0.815789   0.815789\n",
      "macro avg      0.789062  0.698214  0.722917  76.000000\n",
      "weighted avg   0.807566  0.815789  0.798904  76.000000\n",
      "--------------------------------------\n",
      "\n",
      "Results for dataset fold number : 3 on model : BERT_base_uncased\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.961538  0.909091  0.934579  55.000000\n",
      "requirement    0.782609  0.900000  0.837209  20.000000\n",
      "accuracy       0.906667  0.906667  0.906667   0.906667\n",
      "macro avg      0.872074  0.904545  0.885894  75.000000\n",
      "weighted avg   0.913824  0.906667  0.908614  75.000000\n",
      "--------------------------------------\n",
      "\n",
      "Results for dataset fold number : 4 on model : BERT_base_uncased\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.961538  0.909091  0.934579  55.000000\n",
      "requirement    0.782609  0.900000  0.837209  20.000000\n",
      "accuracy       0.906667  0.906667  0.906667   0.906667\n",
      "macro avg      0.872074  0.904545  0.885894  75.000000\n",
      "weighted avg   0.913824  0.906667  0.908614  75.000000\n",
      "--------------------------------------\n",
      "\n",
      "Results for dataset fold number : 5 on model : BERT_base_uncased\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.828125  0.946429  0.883333  56.000000\n",
      "requirement    0.727273  0.421053  0.533333  19.000000\n",
      "accuracy       0.813333  0.813333  0.813333   0.813333\n",
      "macro avg      0.777699  0.683741  0.708333  75.000000\n",
      "weighted avg   0.802576  0.813333  0.794667  75.000000\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "fold_count = 1\n",
    "results = []\n",
    "avg_accuracy = []\n",
    "for subs in sorted(sub_folders):\n",
    "    test_path = subs + '/test_' + 'fold_' + str(fold_count) + '.csv'\n",
    "\n",
    "    df_test = pd.read_csv(test_path)\n",
    "    selected_test = df_test[['STR.REQ','class']]\n",
    "\n",
    "    test_sequences = selected_test['STR.REQ'].tolist()\n",
    "\n",
    "    test_encodings = tokenizer(test_sequences, truncation=True,\n",
    "                               padding=True,\n",
    "                               max_length=MAX_SEQ_LENGTH,\n",
    "                               return_tensors=\"pt\")\n",
    "    # load model\n",
    "    model_path = glob.glob(prefix + model_name + '/fold_' + str(fold_count) + '/*')[0]\n",
    "    bert_model = load_BERT_model(model_name, model_path)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = bert_model(**test_encodings).logits\n",
    "\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    evaluation_results = classification_report(selected_test['class'].tolist(),\n",
    "                                               predictions.tolist(),\n",
    "                                               target_names=list(map_labels.values()),\n",
    "                                               output_dict=True)\n",
    "\n",
    "    avg_accuracy.append(get_accuracy(selected_test['class'].tolist(),\n",
    "                                     predictions.tolist()))\n",
    "\n",
    "    report_df = pd.DataFrame(evaluation_results).transpose()\n",
    "    results.append(report_df)\n",
    "\n",
    "    print('\\nResults for dataset fold number :',fold_count, 'on model :', model_name)\n",
    "    print('\\n',report_df)\n",
    "    print('--------------------------------------')\n",
    "\n",
    "    fold_count += 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for dataset fold number : 1\n",
      "\n",
      " 0.8442\n",
      "\n",
      "Results for dataset fold number : 2\n",
      "\n",
      " 0.7414\n",
      "\n",
      "Results for dataset fold number : 3\n",
      "\n",
      " 0.6991\n",
      "\n",
      "Results for dataset fold number : 4\n",
      "\n",
      " 0.7825\n",
      "\n",
      "Results for dataset fold number : 5\n",
      "\n",
      " 0.7064\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "for row in range (5):\n",
    "\n",
    "    # Get the shape of the arrays\n",
    "    num_rows = len(lr_overall_predicted_labels)\n",
    "    num_cols = len(lr_overall_predicted_labels[row])\n",
    "    # Initialize an empty array for the final result\n",
    "    result_array = np.zeros((num_rows, num_cols), dtype=int)\n",
    "\n",
    "    test_path = './data/dronology_five_folds/fold_' + str(row + 1) + '/test_' + 'fold_' + str(row + 1) + '.csv'\n",
    "\n",
    "    df_test = pd.read_csv(test_path)\n",
    "    df_test['STR.REQ'] = df_test['STR.REQ'].str.lower()\n",
    "    X_test = df_test['STR.REQ']\n",
    "    y_test = df_test['class']\n",
    "    for col in range (len(lr_overall_predicted_labels[row]) - 1):\n",
    "        prediction_sum = lr_overall_predicted_labels[row][col]  + svm_overall_predicted_labels[row][col] + psvm_overall_predicted_labels[row][col]\n",
    "\n",
    "        if prediction_sum >= 1:\n",
    "            result_array[row][col] = 1\n",
    "        else:\n",
    "            result_array[row][col] = 0\n",
    "\n",
    "        evaluation_results = classification_report(y_test.tolist(), result_array.tolist()[row],\n",
    "                                                   target_names=list(map_labels.values()),\n",
    "                                                   output_dict=True)\n",
    "\n",
    "        avg_accuracy.append(get_accuracy(y_test.tolist(), result_array.tolist()[row]))\n",
    "\n",
    "        report_df = pd.DataFrame(evaluation_results).transpose()\n",
    "        ml_results.append(report_df)\n",
    "\n",
    "    print('\\nResults for dataset fold number :',str(row + 1))\n",
    "    print('\\n',round(report_df.loc['weighted avg']['f1-score'], 4))\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
